{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Quenching the Thirst for Insights\n\n*Water is one of the most valueable ressources not only to us humans but for many other living organisms as well. Although, water security often gets less media attention than climate change topics, water scarcity is a serious issue that affects us all. **Water scarcity** is created where the water withdrawal from a basin exceeds its recharge and **water stress level** is the freshwater withdrawal as a proportion of available freshwater resources. Managing water as a ressource can be challenging. Therefore, the goal of this challenge is to predict water levels to help Acea Group preserve precious waterbodies.\nIn this notebook we are getting an overview of the challenge first and then explore the various datasets and see if we can find some interesting insights.*\n\n(Additional information on water scarcity and water stress level can be found [in this notebook](https://www.kaggle.com/iamleonie/impact-potential-analysis-of-water-use-efficiency).)\n\n# Challenge Overview\n\nSince Acea Groups is an Italian multiutility operator, we are looking at datasets containing information about **waterbodies in Italy**. \nThe distribution of datasets for the type of waterbody is different. There are four waterbody types: aquifier, water spring, lake and river. The datasets are not equally distributed over the waterbody types, as shown below:\n> This competition uses nine different datasets, completely independent and not linked to each other. \n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nimport re\nimport datetime\n\nfrom datetime import datetime\nimport matplotlib.cm as cm\nfrom geopy.geocoders import Nominatim\nimport folium\nfrom tqdm import tqdm \n\nfrom math import radians, cos, sin, asin, sqrt\nfrom sklearn.cluster import KMeans\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport gc\nimport lightgbm as lgb\n\n\n\nPATH = \"../input/acea-water-prediction/\"\naquifer_auser_df = pd.read_csv(f\"{PATH}Aquifer_Auser.csv\")\naquifer_doganella_df = pd.read_csv(f\"{PATH}Aquifer_Doganella.csv\")\naquifer_luco_df = pd.read_csv(f\"{PATH}Aquifer_Luco.csv\")\naquifer_petrignano_df = pd.read_csv(f\"{PATH}Aquifer_Petrignano.csv\")\n\nlake_biliancino_df = pd.read_csv(f\"{PATH}Lake_Bilancino.csv\")\n\nriver_arno_df = pd.read_csv(f\"{PATH}River_Arno.csv\")\n\nwater_spring_amiata_df = pd.read_csv(f\"{PATH}Water_Spring_Amiata.csv\")\nwater_spring_lupa_df = pd.read_csv(f\"{PATH}Water_Spring_Lupa.csv\")\nwater_spring_madonna_df = pd.read_csv(f\"{PATH}Water_Spring_Madonna_di_Canneto.csv\")\n\ndatasets = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if '.csv' in filename:\n            datasets += list([filename])\n\ndatasets_df = pd.DataFrame(columns=['filename'], data=datasets)\ndatasets_df['waterbody_type'] = datasets_df.filename.apply(lambda x: x.split('_')[0])\n\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(6,4))\n\nsns.countplot(datasets_df.waterbody_type, palette=['lightblue'])\nax.set_ylabel('Number of Datasets', fontsize=14)\nax.set_xlabel('Waterbody Type', fontsize=14)\nax.set_xticklabels(labels=['Aquifier', 'Water Spring', 'Lake', 'River'], fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we are working with different datasets, we also have different columns.\n> The reality is that each waterbody has such unique characteristics that their attributes are not linked to each other. \n\nLet's check which are column categories the datasets have in common. As shown below, the columns `Date`, `Rainfall` and `Temperature` appear in all datasets. The columns `Lake Level` and `Volume` are waterbody unique columns.\n\nAdditionally, for each waterbody a different feature has to be predicted. In below figure the target variable is marked with a lightblue box.\n\n**The goal is to create four mathematical models, one for each category of waterbody (acquifers, water springs, river, lake) to predict the amount of water in each unique waterbody.**"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"temp_df = pd.DataFrame({'column_name' : aquifer_auser_df.columns, 'waterbody_type':'Aquifier'})\ntemp_df = temp_df.append(pd.DataFrame({'column_name' : aquifer_doganella_df.columns, 'waterbody_type':'Aquifier'}))\ntemp_df = temp_df.append(pd.DataFrame({'column_name' : aquifer_luco_df.columns, 'waterbody_type':'Aquifier'}))\ntemp_df = temp_df.append(pd.DataFrame({'column_name' : aquifer_petrignano_df.columns, 'waterbody_type':'Aquifier'}))\ntemp_df = temp_df.append(pd.DataFrame({'column_name' : lake_biliancino_df.columns, 'waterbody_type':'Lake'}))\ntemp_df = temp_df.append(pd.DataFrame({'column_name' : river_arno_df.columns, 'waterbody_type':'River'}))\ntemp_df = temp_df.append(pd.DataFrame({'column_name' : water_spring_amiata_df.columns, 'waterbody_type':'Water Spring'}))\ntemp_df = temp_df.append(pd.DataFrame({'column_name' : water_spring_lupa_df.columns, 'waterbody_type':'Water Spring'}))\ntemp_df = temp_df.append(pd.DataFrame({'column_name' : water_spring_madonna_df.columns, 'waterbody_type':'Water Spring'}))\n\ndef get_column_category(x):\n    if 'Date' in x:\n        return 'Date'\n    elif 'Rainfall' in x:\n        return 'Rainfall'\n    elif 'Depth' in x:\n        return 'Depth to Groundwater'\n    elif 'Temperature' in x:\n        return 'Temperature'\n    elif 'Volume' in x:\n        return 'Volume'\n    elif 'Hydrometry' in x:\n        return 'Hydrometry'\n    elif 'Lake_Level' in x:\n        return 'Lake Level'\n    elif 'Flow_Rate' in x:\n        return 'Flow Rate'\n    else:\n        return x\n\ntemp_df['column_category'] = temp_df.column_name.apply(lambda x: get_column_category(x))\n\ntemp_df = temp_df.groupby('waterbody_type').column_category.value_counts().to_frame()\ntemp_df.columns = ['amount']\ntemp_df =temp_df.reset_index(drop=False)\ntemp_df = temp_df.pivot(index='waterbody_type', columns='column_category')['amount']\ntemp_df = temp_df.notna()\n\ntemp_df = temp_df[['Date','Rainfall', 'Temperature',  'Depth to Groundwater', 'Flow Rate', 'Hydrometry', 'Lake Level',\n       'Volume']]\n\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,6))\n\nsns.heatmap(temp_df, cmap='Blues', linewidth=1, ax=ax)\n\nax.set_ylabel('Waterbody Type', fontsize=16)\nax.set_xlabel('Column Category', fontsize=16)\nax.set_title('Heatmap of Column Categories in Dataset', fontsize=16)\nax.add_patch(Rectangle((3, 0), 1, 1, fill=True, alpha=1, color='dodgerblue', lw=0))\nax.add_patch(Rectangle((6, 1), 1, 1, fill=True, alpha=1, color='dodgerblue', lw=0))\nax.add_patch(Rectangle((4, 1), 1, 1, fill=True, alpha=1, color='dodgerblue', lw=0))\nax.add_patch(Rectangle((5, 2), 1, 1, fill=True, alpha=1, color='dodgerblue', lw=0))\nax.add_patch(Rectangle((4, 3), 1, 1, fill=True, alpha=1, color='dodgerblue', lw=0))\nax.annotate('Target \\n Value', (3.1, 0.6), color='white', weight='bold', fontsize=14)\nax.annotate('Target \\n Value', (6.1, 1.6), color='white', weight='bold', fontsize=14)\nax.annotate('Target \\n Value', (4.1, 1.6), color='white', weight='bold', fontsize=14)\nax.annotate('Target \\n Value', (5.1, 2.6), color='white', weight='bold', fontsize=14)\nax.annotate('Target \\n Value', (4.1, 3.6), color='white', weight='bold', fontsize=14)\n\nfor tick in ax.xaxis.get_major_ticks():\n    tick.label.set_fontsize(14) \nfor tick in ax.yaxis.get_major_ticks():\n    tick.label.set_fontsize(14)\n    tick.label.set_rotation('horizontal')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing\n\n"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Dropping Rows with Full NaN Values\ndef drop_nan_rows(df):\n    columns = [c for c in df.columns if c != 'Date']\n    df_mod = df[columns]\n    df_mod.dropna(axis = 0, how = 'all', inplace = True)\n    df_mod['Date'] = df['Date']\n    print(f\"Dropped {len(df)-len(df_mod)} rows\")\n    return df_mod\n\n#aquifer_auser_df = drop_nan_rows(aquifer_auser_df)\naquifer_doganella_df = drop_nan_rows(aquifer_doganella_df)\n#aquifer_luco_df = drop_nan_rows(aquifer_luco_df)\n#aquifer_petrignano_df = drop_nan_rows(aquifer_petrignano_df)\n#lake_biliancino_df = drop_nan_rows(lake_biliancino_df)\nriver_arno_df = drop_nan_rows(river_arno_df)\n#water_spring_amiata_df = drop_nan_rows(water_spring_amiata_df)\n#water_spring_lupa_df = drop_nan_rows(water_spring_lupa_df)\nwater_spring_madonna_df = drop_nan_rows(water_spring_madonna_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Finding Duplicate Features\nThe datasets for Lake Biliancino and River Arno share the column names `Rainfall_S_Piero`, `Rainfall_Mangona`, `Rainfall_S_Agata`, `Rainfall_Cavallina`, and `Rainfall_Le_Croci`. These columns also **share the same values**. Therefore, we can assume that these are duplicates.\n\nThe datasets for Aquifer Doganella and Aquifer Luco share the column names `Depth_to_Groundwater_Pozzo_1`, `Depth_to_Groundwater_Pozzo_3`, `Depth_to_Groundwater_Pozzo_4`, `Volume_Pozzo_1`, `Volume_Pozzo_3` and `Volume_Pozzo_4`. These columns **do not share the same values**. Therefore, we can assume that these are not duplicates.\n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"River Arno and Lake Biliancino share the same values for column Rainfall_S_Piero: \", \n      river_arno_df[river_arno_df.Date.isin(lake_biliancino_df.Date.unique()) & river_arno_df.Rainfall_S_Piero.notna()]['Rainfall_S_Piero'].reset_index(drop=True)\\\n        .equals(lake_biliancino_df[lake_biliancino_df.Rainfall_S_Piero.notna()]['Rainfall_S_Piero'].reset_index(drop=True)))\n\nprint(\"River Arno and Lake Biliancino share the same values for column Rainfall_Mangona: \", \n      river_arno_df[river_arno_df.Date.isin(lake_biliancino_df.Date.unique()) & river_arno_df.Rainfall_Mangona.notna()]['Rainfall_Mangona'].reset_index(drop=True)\\\n        .equals(lake_biliancino_df[lake_biliancino_df.Rainfall_Mangona.notna()]['Rainfall_Mangona'].reset_index(drop=True)))\n\nprint(\"River Arno and Lake Biliancino share the same values for column Rainfall_S_Agata: \", \n      river_arno_df[river_arno_df.Date.isin(lake_biliancino_df.Date.unique()) & river_arno_df.Rainfall_S_Agata.notna()]['Rainfall_S_Agata'].reset_index(drop=True)\\\n        .equals(lake_biliancino_df[lake_biliancino_df.Rainfall_S_Agata.notna()]['Rainfall_S_Agata'].reset_index(drop=True)))\n\nprint(\"River Arno and Lake Biliancino share the same values for column Rainfall_Cavallina: \", \n      river_arno_df[river_arno_df.Date.isin(lake_biliancino_df.Date.unique()) & river_arno_df.Rainfall_Cavallina.notna()]['Rainfall_Cavallina'].reset_index(drop=True)\\\n        .equals(lake_biliancino_df[lake_biliancino_df.Rainfall_Cavallina.notna()]['Rainfall_Cavallina'].reset_index(drop=True)))\n\nprint(\"River Arno and Lake Biliancino share the same values for column Rainfall_Le_Croci: \", \n      river_arno_df[river_arno_df.Date.isin(lake_biliancino_df.Date.unique()) & river_arno_df.Rainfall_Le_Croci.notna()]['Rainfall_Le_Croci'].reset_index(drop=True)\\\n        .equals(lake_biliancino_df[lake_biliancino_df.Rainfall_Le_Croci.notna()]['Rainfall_Le_Croci'].reset_index(drop=True)))\n\nprint(\"Aquifer Doganella and Aquifer Luco share the same values for column Depth_to_Groundwater_Pozzo_1: \", \n      aquifer_doganella_df[aquifer_doganella_df.Date.isin(aquifer_luco_df.Date.unique()) & aquifer_doganella_df.Depth_to_Groundwater_Pozzo_1.notna()]['Depth_to_Groundwater_Pozzo_1'].reset_index(drop=True)\\\n        .equals(aquifer_luco_df[aquifer_luco_df.Depth_to_Groundwater_Pozzo_1.notna()]['Depth_to_Groundwater_Pozzo_1'].reset_index(drop=True)))\n\nprint(\"Aquifer Doganella and Aquifer Luco share the same values for column Depth_to_Groundwater_Pozzo_3: \", \n      aquifer_doganella_df[aquifer_doganella_df.Date.isin(aquifer_luco_df.Date.unique()) & aquifer_doganella_df.Depth_to_Groundwater_Pozzo_3.notna()]['Depth_to_Groundwater_Pozzo_3'].reset_index(drop=True)\\\n        .equals(aquifer_luco_df[aquifer_luco_df.Depth_to_Groundwater_Pozzo_3.notna()]['Depth_to_Groundwater_Pozzo_3'].reset_index(drop=True)))\n\nprint(\"Aquifer Doganella and Aquifer Luco share the same values for column Depth_to_Groundwater_Pozzo_4: \", \n      aquifer_doganella_df[aquifer_doganella_df.Date.isin(aquifer_luco_df.Date.unique()) & aquifer_doganella_df.Depth_to_Groundwater_Pozzo_4.notna()]['Depth_to_Groundwater_Pozzo_4'].reset_index(drop=True)\\\n        .equals(aquifer_luco_df[aquifer_luco_df.Depth_to_Groundwater_Pozzo_4.notna()]['Depth_to_Groundwater_Pozzo_4'].reset_index(drop=True)))\n\nprint(\"Aquifer Doganella and Aquifer Luco share the same values for column Volume_Pozzo_1: \", \n      aquifer_doganella_df[aquifer_doganella_df.Date.isin(aquifer_luco_df.Date.unique()) & aquifer_doganella_df.Volume_Pozzo_1.notna()]['Volume_Pozzo_1'].reset_index(drop=True)\\\n        .equals(aquifer_luco_df[aquifer_luco_df.Volume_Pozzo_1.notna()]['Volume_Pozzo_1'].reset_index(drop=True)))\n\nprint(\"Aquifer Doganella and Aquifer Luco share the same values for column Volume_Pozzo_3: \", \n      aquifer_doganella_df[aquifer_doganella_df.Date.isin(aquifer_luco_df.Date.unique()) & aquifer_doganella_df.Volume_Pozzo_3.notna()]['Volume_Pozzo_3'].reset_index(drop=True)\\\n        .equals(aquifer_luco_df[aquifer_luco_df.Volume_Pozzo_3.notna()]['Volume_Pozzo_3'].reset_index(drop=True)))\n\nprint(\"Aquifer Doganella and Aquifer Luco share the same values for column Volume_Pozzo_4: \", \n      aquifer_doganella_df[aquifer_doganella_df.Date.isin(aquifer_luco_df.Date.unique()) & aquifer_doganella_df.Volume_Pozzo_4.notna()]['Volume_Pozzo_4'].reset_index(drop=True)\\\n        .equals(aquifer_luco_df[aquifer_luco_df.Volume_Pozzo_4.notna()]['Volume_Pozzo_4'].reset_index(drop=True)))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"waterbodies_df = aquifer_auser_df.merge(aquifer_doganella_df, on='Date', how='outer')\nwaterbodies_df = waterbodies_df.merge(aquifer_luco_df, on='Date', how='outer')\nwaterbodies_df = waterbodies_df.merge(aquifer_petrignano_df, on='Date', how='outer')\nwaterbodies_df = waterbodies_df.merge(lake_biliancino_df[['Date','Temperature_Le_Croci','Lake_Level', 'Flow_Rate']], on='Date', how='outer') # Only merge specific columns because 'Rainfall_S_Piero', 'Rainfall_Mangona', 'Rainfall_S_Agata', 'Rainfall_Cavallina', 'Rainfall_Le_Croci' are shared with river_arno_df\nwaterbodies_df = waterbodies_df.merge(river_arno_df, on='Date', how='outer')\nwaterbodies_df = waterbodies_df.merge(water_spring_amiata_df, on='Date', how='outer')\nwaterbodies_df = waterbodies_df.merge(water_spring_lupa_df, on='Date', how='outer')\nwaterbodies_df = waterbodies_df.merge(water_spring_madonna_df, on='Date', how='outer')\n\nwaterbodies_df['Date_dt'] = pd.to_datetime(waterbodies_df.Date, format = '%d/%m/%Y')\nwaterbodies_df = waterbodies_df.sort_values(by='Date_dt').reset_index(drop=True)\nwaterbodies_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis\n\nIn this section, we are going to explore the datasets.\n\n## Geographical Analysis\nNext, let's plot the locations for which we have rainfall and temperature data to get a feeling for the data. For this, we will **retrieve the coordinates** for each location. Next, we will **cluster the locations by KMeans** according to their coordinates and plot them on a map."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"geolocator = Nominatim(user_agent='myapplication')\n\ncity_locations_df = pd.DataFrame(columns=['city', 'lat', 'lon'] )\nfor city in waterbodies_df.columns[waterbodies_df.columns.str.startswith('Temperature')]:\n    city = city.split('Temperature_')[1]\n        \n    city_dict = {}\n    city_dict['city'] = city\n\n    city = re.sub('_', ' ', city)\n    city = re.sub('S ', 'San ', city)\n    # Manunal corrections of city names for geolocator\n    if city == 'San Agata':\n        city = \"Sant’Agata\"\n    try:\n        location = geolocator.geocode(f\"{city}, Italy\")\n        #folium.Marker([location.raw['lat'],location.raw['lon']], popup=city, icon=folium.Icon(color='blue')).add_to(m)\n        city_dict['lat'] = location.raw['lat']\n        city_dict['lon'] = location.raw['lon']\n    except:\n        # Manual coordinates for missing cities:\n        if city == 'Monteroni Arbia Biena':\n            city_dict['lat'] = 43.228279\n            city_dict['lon'] = 11.4021433\n    \n    # Manual coordinates for wrong locations:\n    if city == 'Monte Serra':\n        city_dict['lat'] = 43.750833\n        city_dict['lon'] = 10.555278\n    elif city == 'Laghetto Verde':\n        city_dict['lat'] = 44.5104966\n        city_dict['lon'] = 10.4032786\n    city_locations_df = city_locations_df.append(city_dict, ignore_index=True)\n\nfor city in waterbodies_df.columns[waterbodies_df.columns.str.startswith('Rainfall')]:\n    city = city.split('Rainfall_')[1]\n\n    city_dict = {}\n    city_dict['city'] = city\n    \n    city = re.sub('_', ' ', city)\n    city = re.sub('S ', 'San ', city)\n    # Manunal corrections of city names for geolocator\n    if city == 'San Agata':\n        city = \"Sant’Agata\"\n    try:\n        location = geolocator.geocode(f\"{city}, Italy\")\n        #folium.Marker([location.raw['lat'],location.raw['lon']], popup=city, icon=folium.Icon(color='blue')).add_to(m)\n        city_dict['lat'] = location.raw['lat']\n        city_dict['lon'] = location.raw['lon']\n    except:\n        # print(f'Could not find coordinates for {city}')\n        # Manual coordinates for missing cities:\n        if city == 'Monticiano la Pineta':\n            city_dict['lat'] = 43.1335066\n            city_dict['lon'] = 11.2408464\n        elif city == 'Ponte Orgia':\n            city_dict['lat'] = 43.2074581\n            city_dict['lon'] = 11.2504416\n        elif city == 'Monteroni Arbia Biena':\n            city_dict['lat'] = 43.228279\n            city_dict['lon'] = 11.4021433\n            \n    # Manual coordinates for wrong locations:\n    if city == 'Monte Serra':\n        city_dict['lat'] = 43.750833\n        city_dict['lon'] = 10.555278\n    elif city == 'Laghetto Verde':\n        city_dict['lat'] = 44.5104966\n        city_dict['lon'] = 10.4032786\n            \n    city_locations_df = city_locations_df.append(city_dict, ignore_index=True)\n\n# Cities with both temperature and rainfall data\n#temp = city_locations_df.city.value_counts().to_frame()\n#print(list(temp[temp.city==2].index.values))\n\ncity_locations_df = city_locations_df.sort_values(by='city').drop_duplicates().reset_index(drop=True)\ncity_locations_df.lat = city_locations_df.lat.astype(float)\ncity_locations_df.lon = city_locations_df.lon.astype(float)\n\n# Cluster locations\nN_CLUSTERS = 18\nkmeans = KMeans(n_clusters=N_CLUSTERS, random_state=42).fit(city_locations_df[['lat', 'lon']]) #1\ncity_locations_df['cluster'] = kmeans.labels_\n\n# Re-arrange clusters by lat/lon\ncity_locations_df['cluster_center_lat'] = city_locations_df['cluster'].apply(lambda x: kmeans.cluster_centers_[x,0])\ncity_locations_df['cluster_center_lon'] = city_locations_df['cluster'].apply(lambda x:kmeans.cluster_centers_[x, 1])\ncity_locations_df = city_locations_df.sort_values(by='cluster').reset_index(drop=True)\ncity_locations_df = city_locations_df.sort_values(by=['cluster_center_lat','cluster_center_lon']).reset_index(drop=True)\n\n# Rename clusters in order \ncluster_dict = {}\nfor old, new in enumerate(city_locations_df.cluster.unique()):\n    cluster_dict[new] = old\n\ncity_locations_df.cluster = city_locations_df.cluster.replace(cluster_dict)\n\n# Plot on map\nm = folium.Map(location=[41.8719, 12.5674], tiles='cartodbpositron',zoom_start=5)\n\ncolors = ['beige', 'orange','pink', 'lightred','red',  'darkred','purple','darkpurple', \n          'lightblue', 'cadetblue', 'blue', 'darkblue',\n         'lightgreen', 'green', 'darkgreen', 'black', 'gray','lightgray', 'white']\n\n\ngeolocator = Nominatim(user_agent='myapplication')\nfor i in city_locations_df.index:\n    folium.Marker([city_locations_df.iloc[i].lat, \n                  city_locations_df.iloc[i].lon],\n                  popup=city_locations_df.iloc[i].city, \n                  icon=folium.Icon(color=colors[city_locations_df.iloc[i].cluster])).add_to(m)\n\nm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we will calculate the **distances between the locations with the haversine formula** and verify the clusters."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def haversine(lon1, lat1, lon2, lat2):\n    \"\"\"\n    Function copied from https://stackoverflow.com/questions/4913349/haversine-formula-in-python-bearing-and-distance-between-two-gps-points\n    Calculate the great circle distance between two points \n    on the earth (specified in decimal degrees)\n    \"\"\"\n\n    R = 6372.8 # Earth radius in kilometers\n\n    dLat = radians(lat2 - lat1)\n    dLon = radians(lon2 - lon1)\n    lat1 = radians(lat1)\n    lat2 = radians(lat2)\n\n    a = sin(dLat/2)**2 + cos(lat1)*cos(lat2)*sin(dLon/2)**2\n    c = 2*asin(sqrt(a))\n\n    return round(R * c, 0)\n\ndistances = np.zeros((len(city_locations_df.index), len(city_locations_df.index)))\nfor i in (city_locations_df.index):\n    for j in city_locations_df.index:\n        distances[i, j] = haversine(city_locations_df.iloc[i].lon, \n                                    city_locations_df.iloc[i].lat, \n                                    city_locations_df.iloc[j].lon, \n                                    city_locations_df.iloc[j].lat)\n\ndistances_df = pd.DataFrame(columns = city_locations_df.city.values, index= city_locations_df.city.values, data=distances)\n\n\n\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(16, 12))\nax.set_title('Clustered Distances between Locations [km]', fontsize=16)\nsns.heatmap(distances_df, cmap='Blues')\nax.add_patch(Rectangle((0, 0), 1, 1, fill=False, alpha=1, color='Black', lw=2)) # Cluster 0\nax.add_patch(Rectangle((1, 1), 1, 1, fill=False, alpha=1, color='Black', lw=2)) # Cluster 1\nax.add_patch(Rectangle((2, 2), 1, 1, fill=False, alpha=1, color='Black', lw=2)) # Cluster 2\nax.add_patch(Rectangle((3, 3), 2, 2, fill=False, alpha=1, color='Black', lw=2)) # Cluster 3\nax.add_patch(Rectangle((5, 5), 1, 1, fill=False, alpha=1, color='Black', lw=2)) # Cluster 4\nax.add_patch(Rectangle((6, 6), 3, 3, fill=False, alpha=1, color='Black', lw=2)) # Cluster 5\nax.add_patch(Rectangle((9, 9), 1, 1, fill=False, alpha=1, color='Black', lw=2)) # Cluster 6\nax.add_patch(Rectangle((10, 10), 1, 1, fill=False, alpha=1, color='Black', lw=2)) # Cluster 7\nax.add_patch(Rectangle((11, 11), 10, 10, fill=False, alpha=1, color='Black', lw=2)) # Cluster 8\nax.add_patch(Rectangle((21, 21), 1, 1, fill=False, alpha=1, color='Black', lw=2)) # Cluster 9\nax.add_patch(Rectangle((22, 22), 3, 3, fill=False, alpha=1, color='Black', lw=2)) # Cluster 10\nax.add_patch(Rectangle((25, 25), 1, 1, fill=False, alpha=1, color='Black', lw=2)) # Cluster 11\nax.add_patch(Rectangle((26, 26), 4, 4, fill=False, alpha=1, color='Black', lw=2)) # Cluster 12\nax.add_patch(Rectangle((30, 30), 5, 5, fill=False, alpha=1, color='Black', lw=2)) # Cluster 13\nax.add_patch(Rectangle((35, 35), 4, 4, fill=False, alpha=1, color='Black', lw=2)) # Cluster 14\nax.add_patch(Rectangle((39, 39), 7, 7, fill=False, alpha=1, color='Black', lw=2)) # Cluster 15\nax.add_patch(Rectangle((46, 46), 1, 1, fill=False, alpha=1, color='Black', lw=2)) # Cluster 16\nax.add_patch(Rectangle((47, 47), 1, 1, fill=False, alpha=1, color='Black', lw=2)) # Cluster 17\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n## Seasonality\n\nThe challenge description gives us a hint about seasonality. Therefore, we will analyse the seasonality of the features in this section. For this purpose, we will **create time features: `year`, `month`, `day_in_year`, `week_in_year`.** \n\n> During fall and winter waterbodies are refilled, but during spring and summer they start to drain.\n\n\nBefore, we look at the seasonality, let's get a feeling about how many years of data we are looking at for each dataset. For the datasets for aquifier Auser, aquifier Luco, river Arno, and water spring Amiata, we have **more than 20 years of data.** On the other side, water spring Madonna only contains information about the past 8 years."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def get_time_features(df):\n    df['year'] = df.Date.apply(lambda x: x.split('/')[2] if x==x else x)\n    df['month'] = df.Date.apply(lambda x: x.split('/')[1]if x==x else x)\n    df['day_in_year'] = df.Date.apply(lambda x: datetime(int(x.split('/')[2]), int(x.split('/')[1]), int(x.split('/')[0]), 0, 0).timetuple().tm_yday if x==x else x)\n    df['week_in_year'] = df.day_in_year.apply(lambda x: int(x/7) if x==x else x)\n    return df\n\naquifer_auser_df = get_time_features(aquifer_auser_df)\naquifer_doganella_df = get_time_features(aquifer_doganella_df)\naquifer_luco_df = get_time_features(aquifer_luco_df)\naquifer_petrignano_df = get_time_features(aquifer_petrignano_df)\nlake_biliancino_df = get_time_features(lake_biliancino_df)\nriver_arno_df = get_time_features(river_arno_df)\nwater_spring_amiata_df = get_time_features(water_spring_amiata_df)\nwater_spring_lupa_df = get_time_features(water_spring_lupa_df)\nwater_spring_madonna_df = get_time_features(water_spring_madonna_df)\n\n\ntemp_df = (pd.DataFrame({'year' : aquifer_auser_df.year.unique(), 'dataset': 'Aquifier Auser'}))\ntemp_df = temp_df.append(pd.DataFrame({'year' : aquifer_doganella_df.year.unique(), 'dataset': 'Aquifier Doganella'}))\ntemp_df = temp_df.append(pd.DataFrame({'year' : aquifer_luco_df.year.unique(), 'dataset': 'Aquifier Luco'}))\ntemp_df = temp_df.append(pd.DataFrame({'year' : aquifer_petrignano_df.year.unique(), 'dataset': 'Aquifier Petrignano'}))\ntemp_df = temp_df.append(pd.DataFrame({'year' : lake_biliancino_df.year.unique(), 'dataset': 'Lake Biliancino'}))\ntemp_df = temp_df.append(pd.DataFrame({'year' : river_arno_df.year.unique(), 'dataset': 'River Arno'}))\ntemp_df = temp_df.append(pd.DataFrame({'year' : water_spring_amiata_df.year.unique(), 'dataset': 'Water Spring Amiata'}))\ntemp_df = temp_df.append(pd.DataFrame({'year' : water_spring_lupa_df.year.unique(), 'dataset': 'Water Spring Lupa'}))\ntemp_df = temp_df.append(pd.DataFrame({'year' : water_spring_madonna_df.year.unique(), 'dataset': 'Water Spring  Madonna'}))\n\ntemp_df['dummy'] = 1\ntemp_df = temp_df[temp_df.year.notna()]\ntemp_df = temp_df.pivot(index='dataset', columns='year')['dummy']\n\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(20,6))\n\nsns.heatmap(temp_df, cmap='Blues', linewidth=1, ax=ax, vmin=0, vmax=1)\n\n\nax.set_ylabel('Dataset', fontsize=16)\nax.set_xlabel('Year', fontsize=16)\nax.set_title('Yearly Data Availability for each Dataset', fontsize=16)\nfor tick in ax.xaxis.get_major_ticks():\n    tick.label.set_fontsize(14) \nfor tick in ax.yaxis.get_major_ticks():\n    tick.label.set_fontsize(14) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Missing Data\nWe can see that there is a lot of missing data in the merged dataset. For simplification reasons, we will drop all data before 2000 since there are only 5 features available. Losing the data is 1998 and 1999 is not evaluated as cricital since there is still 20 years of data left."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"waterbodies_df['year'] = waterbodies_df.Date.apply(lambda x: x.split('/')[2] if x==x else x)\nwaterbodies_df['year'] = waterbodies_df['year'].astype(int)\n\ntemp = waterbodies_df[[c for c in waterbodies_df.columns if c != 'year']].isna().astype(int)\ntemp['year'] = waterbodies_df.year\ntemp = temp.groupby('year').mean()\n\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(30,6))\n\nsns.heatmap(temp,  \n            cmap='binary', vmin=-0.1, vmax=1, ax=ax, linewidth=1)#[0])\nax.set_ylabel('Year', fontsize=16)\nax.set_xlabel('Feature', fontsize=16)\nax.set_title('Missing Values', fontsize=16)\n\nplt.show()\n\nwaterbodies_df = waterbodies_df[waterbodies_df.year>= 2000].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing Values and Implausible Zero Values\nFrom below plot, we can see that the temperature and the rainfall columns have a lot of missing values (NaN). Also, we can see that there are a lot **implausible temperature values of 0°C**. For the rainfall data, the zero values seem plausible."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"f, ax = plt.subplots(nrows=1, ncols=2, figsize=(24,6))\n\nsns.heatmap(waterbodies_df[waterbodies_df.columns[waterbodies_df.columns.str.startswith('Rainfall')]].isna().astype(int),  \n            cmap='binary', linewidth=0, vmin=0, vmax=1, ax=ax[0])\nax[0].set_ylabel('Row', fontsize=16)\nax[0].set_xlabel('Column', fontsize=16)\nax[0].set_title('Missing Rainfall Values', fontsize=16)\n\nsns.heatmap(waterbodies_df[waterbodies_df.columns[waterbodies_df.columns.str.startswith('Temperature')]].isna().astype(int),  \n            cmap='binary', linewidth=0, vmin=0, vmax=1, ax=ax[1])\nax[1].set_ylabel('Row', fontsize=16)\nax[1].set_xlabel('Year', fontsize=16)\nax[1].set_title('Missing Temperature Values', fontsize=16)\n\nplt.show()\n\nf, ax = plt.subplots(nrows=1, ncols=2, figsize=(24,6))\n\nsns.heatmap((waterbodies_df[waterbodies_df.columns[waterbodies_df.columns.str.startswith('Rainfall')]] == 0).astype(int),  \n            cmap='binary', linewidth=0, vmin=0, vmax=1, ax=ax[0])\nax[0].set_ylabel('Row', fontsize=16)\nax[0].set_xlabel('Column', fontsize=16)\nax[0].set_title('Zero Rainfall Values', fontsize=16)\n\nsns.heatmap((waterbodies_df[waterbodies_df.columns[waterbodies_df.columns.str.startswith('Temperature')]] == 0).astype(int),  \n            cmap='binary', linewidth=0, vmin=0, vmax=1, ax=ax[1])\nax[1].set_ylabel('Row', fontsize=16)\nax[1].set_xlabel('Year', fontsize=16)\nax[1].set_title('Zero Temperature Values', fontsize=16)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's have a look at the **seasonality of rainfall and temperature** first. Below plots show the overall mean rainfall and temperature over the weeks in a year. We can see that on average, the **rainfall is low in the summer months and higher in the winter months**. There is a **peak of rainfall around November**. The temperature has an almost opposite seasonality. The **temperature reaches a peak in summer**."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"temperature_cols = waterbodies_df.columns[waterbodies_df.columns.str.startswith('Temperature')]\nwaterbodies_df[temperature_cols] = waterbodies_df[temperature_cols].replace({0 : np.nan})\n\nwaterbodies_df = get_time_features(waterbodies_df)\nwaterbodies_df.month = waterbodies_df.month.astype(int)\n\ncolumns = waterbodies_df.columns[waterbodies_df.columns.str.startswith('Rainfall') | waterbodies_df.columns.str.startswith('Temperature')]\n\ntemp = waterbodies_df.groupby('week_in_year')[columns].mean().reset_index(drop=False)\n\nf, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\nf.suptitle('Seasonality of Rainfall and Temperature', fontsize=16)\n\nevenly_spaced_interval = np.linspace(0, 1, len(temp.columns[temp.columns.str.startswith('Rainfall')])+1)\ncolors = [cm.Spectral(x) for x in evenly_spaced_interval]\nfor i, col in enumerate(temp.columns[temp.columns.str.startswith('Rainfall')]):\n    sns.lineplot(data=temp, x='week_in_year', y=col, ax=ax[0], label=col, color=colors[i+1])\nax[0].legend(bbox_to_anchor=(0.6, -0.1, 0, 0))\nax[0].set_ylabel('Rainfall', fontsize=14)\nax[0].set_xlabel('Week in Year', fontsize=14)\nax[0].set_ylim([0, 20])\n\nevenly_spaced_interval = np.linspace(0, 1, len(temp.columns[temp.columns.str.startswith('Temperature')])+1)\ncolors = [cm.Spectral(x) for x in evenly_spaced_interval]\nfor i, col in enumerate(temp.columns[temp.columns.str.startswith('Temperature')]):\n    sns.lineplot(data=temp, x='week_in_year', y=col, ax=ax[1], label=col, color=colors[i+1])\nax[1].legend(bbox_to_anchor=(0.6, -0.1, 0, 0))\nax[1].set_ylabel('Temperature [°C]', fontsize=14)\nax[1].set_xlabel('Week in Year', fontsize=14)  \nax[1].set_ylim([0, 30])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For Velletri, Settefrati, Pentolina, Monteroni Arbia Biena, Laghetto Verde, Le Croci, Abbadia San Salvatore, San Fiora, Bastia Umbra, Mensano, Orentano, Monte Serra, Monteporzio, and Siena Poggio al Vento, we have **both rainfall and temperature data**.\nThe mean values of **Siena Poggio al Vento, and Mensano seem implausible** since they are lower than the temperatures in other Italian location. This could however be caused by the implausible zero values as seen in above section.\nWater spring Lupa, could be the most challenging to predict since it only has two feature (`Date` and `Rainfall_Terni`)."},{"metadata":{},"cell_type":"markdown","source":"### Imputing Temperature Data by Prediction\nAs a preprocessing step of the data, we will fill the NaN values and replace the implausible values as well.\nFirst, the implausible values of 0°C will be replaced with NaN values. Then all NaN values will be filled by prediction. For this, we will build a small prediction model.\n\nWe will be using LightGBM with a 5 fold CV. The features are the temperature columns as well as the date features.\nAlthough, the mean squared error is quite high for the below shown example, we can see that the rolling mean for the predicted values is very close to the rolling mean of the ground truth. Although the models performance is not very good, we can evaluate it as sufficient for filling the missing values.\n\n(Work in progress: Maybe utilize above clusters as a second model to improve prediction)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import datetime\n\nparams = {'num_leaves': 24,\n          'objective': 'regression',\n          'max_depth': 16,\n          'learning_rate': 0.005,\n          \"metric\": 'rmse',\n          \"verbosity\": -1,\n          'random_state': 42,\n          'verbose': -1,\n         }\n\nNFOLDS = 5\n\ndef predict_missing_temperature(target, debug=False):\n\n    if debug:\n        f, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 6))\n        ax.set_title(f'Temperatures for {target}', fontsize=16)\n\n        sns.lineplot(x=waterbodies_df.Date_dt, y=waterbodies_df[target], label=target, )\n        sns.scatterplot(x=waterbodies_df.Date_dt, y=waterbodies_df[target].isna().apply(lambda x: 15 if x else np.nan), color='red', linewidth=0, label='To predict' )\n        ax.set_xlim([datetime.date(2000, 1, 1), datetime.date(2020, 6, 30)])\n        plt.show()\n\n    test_df = waterbodies_df.copy()\n    train_df = waterbodies_df[waterbodies_df[target].notna()]\n\n    features = [c for c in (list(temperature_cols) + list(['month', 'day_in_year', 'week_in_year'])) if c != target]\n\n    X = train_df[features]\n    y = train_df[[target]]\n    X_test = test_df[features]\n\n    folds = KFold(n_splits=NFOLDS)\n    splits = folds.split(X, y)\n    y_preds = np.zeros(X_test.shape[0])\n    y_oof = np.zeros(X.shape[0])\n    score = 0\n\n    for fold_n, (train_index, valid_index) in enumerate(splits):\n        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n\n        dtrain = lgb.Dataset(X_train, y_train, params= {'verbose': -1})\n        dvalid = lgb.Dataset(X_valid, y_valid, params= {'verbose': -1})\n\n        # For analysis set 'verbose_eval' to 200\n        clf = lgb.train(params, dtrain, 10000, valid_sets = [dtrain, dvalid], verbose_eval=False,  early_stopping_rounds=300)\n\n        y_pred_valid = clf.predict(X_valid)\n        y_oof[valid_index] = y_pred_valid\n        #print(f\"Fold {fold_n + 1} | RSME: {mean_squared_error(y_valid.values.reshape(-1), y_pred_valid)}\")\n        y_preds += clf.predict(X_test) / NFOLDS\n\n        del X_train, X_valid, y_train, y_valid\n        gc.collect()\n\n    #print(f\"Predicting target {target} with RSME: {mean_squared_error(y, y_oof)}\")\n    if debug:\n        f, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 4))\n        ax.set_title(f'Rolling Mean Temperatures for {target} \\n(Rolling Window: 28 Days, RSME: {mean_squared_error(y, y_oof)})', fontsize=16)\n        sns.lineplot(x=waterbodies_df.Date_dt, y=pd.Series(y_preds).rolling(28).mean(), label='Predicted', color='tomato')\n        sns.lineplot(x=waterbodies_df.Date_dt, y=waterbodies_df[target].rolling(28).mean(), label='Ground Truth', color='dodgerblue')\n        ax.set_xlabel('Date', fontsize=14)\n        ax.set_xlim([datetime.date(2000, 1, 1), datetime.date(2020, 6, 30)])\n        plt.show()\n    return pd.Series(y_preds)\n\nfor target in temperature_cols:\n    waterbodies_df[target] = np.where(waterbodies_df[target].isna(), predict_missing_temperature(target, True), waterbodies_df[target])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting the rolling mean temperature values shows that in 2014 the temperature values for `Temperature_Siena_Poggio_al_Vento`, `Temperature_Mensano`, and `Temperature_Pentolina` is irregular incomparison to other locations and in comparison to the other years."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 6))\nevenly_spaced_interval = np.linspace(0, 1, len(temperature_cols))#+1)\ncolors = [cm.Spectral(x) for x in evenly_spaced_interval]\nfor i, target in enumerate(temperature_cols):\n    sns.lineplot(x=waterbodies_df.Date_dt, y=waterbodies_df[target].rolling(28).mean(), label=target, color=colors[i])#+1])\n    ax.set_xlim([datetime.date(2000, 1, 1), datetime.date(2020, 6, 30)])\n\nplt.legend(bbox_to_anchor = (1.2,0.5), loc = \"center\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Work in Progress..."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"'''\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 4))\nax.set_title(f'Rolling Mean Temperatures for {target} \\n(Rolling Window: 28 Days, RSME: {mean_squared_error(y, y_oof)})', fontsize=16)\nsns.lineplot(x=waterbodies_df.Date_dt, y=waterbodies_df.Lake_Level, label='Ground Truth', color='dodgerblue')\nax.set_xlabel('Date', fontsize=14)\nax.set_xlim([datetime.date(1998, 1, 1), datetime.date(2020, 6, 30)])\nplt.show()\n\n\n\naquifer_auser_df \naquifer_doganella_df \naquifer_luco_df\naquifer_petrignano_df\nlake_biliancino_df\nriver_arno_df\nwater_spring_amiata_df\nwater_spring_lupa_df\nwater_spring_madonna_df\n\n'''\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}